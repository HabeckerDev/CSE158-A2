{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "from gensim.models import Word2Vec\n",
    "import dateutil\n",
    "from scipy.sparse import lil_matrix # To build sparse feature matrices, if you like\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = json.load(open(\"train-transcripts-aligned.json\", 'r+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4099999999999999\n"
     ]
    }
   ],
   "source": [
    "print(dataset['ep-1'][0]['duration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_length = []\n",
    "total_length = []\n",
    "statement_length = []\n",
    "data = dataset['ep-1']\n",
    "\n",
    "for d in data:\n",
    "    if d['has_q'] is True:\n",
    "        question_length.append(d['duration'])\n",
    "    else:\n",
    "        statement_length.append(d['duration'])\n",
    "    total_length.append(d['duration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801.5500000000005 2723.4133333333316 3524.9633333333322\n",
      "3.397683654585902\n",
      "4.397683654585903\n",
      "average q,s,t:  15.414423076923088 19.878929440389282 18.650599647266308\n",
      "0.7754151511602343\n"
     ]
    }
   ],
   "source": [
    "# question utterance duration is about 0.775 times shorter than statement utterance duration\n",
    "q_length = sum(question_length)\n",
    "t_length = sum(total_length)\n",
    "s_length = sum(statement_length)\n",
    "print(q_length, s_length, t_length)\n",
    "print(s_length/q_length)\n",
    "print(t_length/q_length)\n",
    "\n",
    "q_average = q_length/len(question_length)\n",
    "s_average = s_length/len(statement_length)\n",
    "t_average = t_length/len(total_length)\n",
    "print('average q,s,t: ', q_average, s_average, t_average)\n",
    "print(q_average/s_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X sample: [[13]\n",
      " [10]\n",
      " [20]\n",
      " [38]\n",
      " [32]]\n",
      "y sample: [0.41 0.81 1.45 1.61 1.44]\n"
     ]
    }
   ],
   "source": [
    "utterance_lengths = []\n",
    "durations = []\n",
    "\n",
    "# Iterating through each episode\n",
    "for episode_id, turns in dataset.items():\n",
    "    # Iterating through each turn in the episode\n",
    "    for turn in turns:\n",
    "        utterance_length = len(turn['utterance'])\n",
    "        duration = turn['utterance_end'] - turn['utterance_start']\n",
    "\n",
    "        utterance_lengths.append(utterance_length)\n",
    "        durations.append(duration)\n",
    "        \n",
    "X = np.array(utterance_lengths).reshape(-1, 1)  # Features (utterance lengths)\n",
    "y = np.array(durations)  # Target (durations)\n",
    "\n",
    "print(\"X sample:\", X[:5])  # Print first 5 samples of features\n",
    "print(\"y sample:\", y[:5])  # Print first 5 samples of targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.7533362253585961\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model (optional)\n",
    "score = model.score(X_test, y_test)\n",
    "print(\"Model accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_lengths = []\n",
    "durations = []\n",
    "has_questions = []  # List to store 'has_q' feature\n",
    "\n",
    "# Iterating through each episode\n",
    "for episode_id, turns in dataset.items():\n",
    "    # Iterating through each turn in the episode\n",
    "    for turn in turns:\n",
    "        utterance_length = len(turn['utterance'])\n",
    "        duration = turn['utterance_end'] - turn['utterance_start']\n",
    "        has_question = int(turn['has_q'])  # Convert boolean to int (True to 1, False to 0)\n",
    "\n",
    "        utterance_lengths.append(utterance_length)\n",
    "        durations.append(duration)\n",
    "        has_questions.append(has_question)\n",
    "        \n",
    "X2 = np.column_stack((utterance_lengths, has_questions))  # Combine utterance lengths and has_q features\n",
    "y2 = np.array(durations)  # Target (durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.7533167433119695\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X2_train, y2_train)\n",
    "\n",
    "# Test the model (optional)\n",
    "score2 = model.score(X2_test, y2_test)\n",
    "print(\"Model accuracy:\", score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
